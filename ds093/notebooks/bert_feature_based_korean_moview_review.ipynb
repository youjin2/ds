{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95293003-f6d3-4e87-8f07-fe4a3c042ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b73578d-8656-4a77-9671-ea1e3e035617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 11:44:50.030366: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from src.datasets.kmreview import load_korean_moview_review\n",
    "from src.utils.logger import get_logger\n",
    "from src.utils.session import reset_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f2eaead-1365-4333-9696-5a3c4adf6551",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9947780-8701-4da9-9151-ca084ae173af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a194252-2b57-4c0f-a32a-d4abf70b141e",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e38ce2db-97e5-4ac0-aa64-f799e4211b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] num train: 6400 (data.py:33)\n",
      "[INFO] num valid: 1600 (data.py:34)\n",
      "[INFO] num test: 2000 (data.py:35)\n"
     ]
    }
   ],
   "source": [
    "(X_train_raw, y_train_raw), (X_valid_raw, y_valid_raw), (X_test_raw, y_test_raw) = load_korean_moview_review(\n",
    "    num_sample=10000,\n",
    "    val_split=True,\n",
    ")\n",
    "\n",
    "X_raw = {\n",
    "    \"X_train\": X_train_raw,\n",
    "    \"X_valid\": X_valid_raw,\n",
    "    \"X_test\": X_test_raw,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1694b46-66ab-44e7-98c7-1dfc8b847689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "595eddd2-e475-4941-a7d7-ff011079d697",
   "metadata": {},
   "source": [
    "## Preproess\n",
    "<!-- - we will use last hidden states to  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49e58d8-02b4-418e-9530-093fee536249",
   "metadata": {},
   "source": [
    "### Tokenize input sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c53345e8-774f-4a82-bc1e-b1b357f1e811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28aaddc9-3be0-47d6-a154-e74754d525ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Args:\n",
    " - return_tensors: returns numpy arra\n",
    " - max_length: limit the maximum length of sentence as 30\n",
    " - padding: fill with zeros if len(sentence) < max_length\n",
    " - truncation: truncate if len(sentence) > max_length\n",
    "\n",
    "Outputs:\n",
    " - input_ids: tokenized input ids\n",
    " - token_type_ids: 0 if first sentence, 1 if second sentence (note: BERT takes 2 sentences as input)\n",
    " - attention_mask: 0 if input_ids[i] = [PAD], 1 otherwise\n",
    "\"\"\"\n",
    "\n",
    "X_tokenized = {\n",
    "    k: tokenizer(v.tolist(), return_tensors=\"np\", max_length=30, padding=\"max_length\", truncation=True)\n",
    "    for k, v in X_raw.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b04b05e4-897a-4693-b229-d76e2cfcfc41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>\n",
      "Raw:  고산 귀신이산다 이장 군수 연기하냐\n",
      "Token ids: [  101 47468 22214  1163 38365 23918 11112 22214 12261 12398 14509 53543\n",
      " 15783  1174 43107 13130 35132 97071 85091   102     0     0     0     0\n",
      "     0     0     0     0     0     0]\n",
      "Ids to token: [CLS] 고 ##산 ᄀ ##ᅱ ##신 ##이 ##산 ##다 이 ##장 군 ##수 ᄋ ##ᅧᆫ ##기 ##하 ##ᄂ ##ᅣ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "==>\n",
      "Raw:  방광 싸움 정도 한장 면도 놓치 싫었\n",
      "Token ids: [  101  1170 47328 12211 13045  1173 25539 27235 13503 81463 12265 17463\n",
      " 14509  1169 43107 12265  1165 29347 97109 18893 33401 97102 13413 97104\n",
      "   102     0     0     0     0     0]\n",
      "Ids to token: [CLS] ᄇ ##ᅡᆼ ##과 ##ᆼ ᄊ ##ᅡ ##우 ##ᆷ 정 ##도 한 ##장 ᄆ ##ᅧᆫ ##도 ᄂ ##ᅩ ##ᇂ ##치 시 ##ᆶ ##어 ##ᆻ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0]\n",
      "\n",
      "==>\n",
      "Raw:  저 저런 다른 볼걸\n",
      "Token ids: [  101  1175 33645  1175 33645 60532 20066 54484 40815 12397   102     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0]\n",
      "Ids to token: [CLS] ᄌ ##ᅥ ᄌ ##ᅥ ##런 다른 볼 ##거 ##ᆯ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "==>\n",
      "Raw:  배우 모두 연기 굿 왜 김혜수 하는 알 되는 영화 마블리 팬 너무 좋\n",
      "Token ids: [  101 80586 29558  1174 43107 13130 54602 18463  1174 83955  1163 95114\n",
      " 97087 61701 15783 24299 74793 53408 41912  1169 25539 87022 13926  1180\n",
      " 65565  1165 33645 32261  1175   102]\n",
      "Ids to token: [CLS] 배우 모두 ᄋ ##ᅧᆫ ##기 구 ##ᆺ ᄋ ##ᅫ ᄀ ##ᅵᆷ ##ᄒ ##ᅨ ##수 하는 알 되는 영화 ᄆ ##ᅡ ##블 ##리 ᄑ ##ᅢᆫ ᄂ ##ᅥ ##무 ᄌ [SEP]\n",
      "Attention Mask: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [0, 10, 13, 20]:\n",
    "    tmp_raw = X_raw[\"X_train\"][i]\n",
    "    tmp_tokenized = X_tokenized[\"X_train\"]\n",
    "    tmp_ids = tmp_tokenized[\"input_ids\"][i]\n",
    "    tmp_token_type_ids = tmp_tokenized[\"token_type_ids\"][i]\n",
    "    tmp_att_msk = tmp_tokenized[\"attention_mask\"][i]\n",
    "\n",
    "    print(\"==>\")\n",
    "    print(f\"Raw: {tmp_raw}\")\n",
    "    print(f\"Token ids: {tmp_ids}\")\n",
    "    print(f\"Ids to token: {' '.join(tokenizer.convert_ids_to_tokens(tmp_ids))}\")\n",
    "    print(f\"Attention Mask: {tmp_att_msk}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef84fe9a-ca4d-44eb-809c-4bcd51de1aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076a2126-8dac-45c2-8956-13a48f483dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe46e068-c39c-4df3-a54b-1e9b0a914cc3",
   "metadata": {},
   "source": [
    "### Retrieve Last Hidden States / Pooler Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1d94139-eca4-4abc-a980-df1adb6f496a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28faeb47-7651-4f24-93fd-205c265e79b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-21 11:44:55.790331: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-21 11:44:55.792143: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-21 11:44:55.792260: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-21 11:44:55.792538: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-21 11:44:55.792859: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-21 11:44:55.792961: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-21 11:44:55.793052: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-21 11:44:56.144611: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-21 11:44:56.144753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-21 11:44:56.144853: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2024-12-21 11:44:56.144939: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6660 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "model = TFBertModel.from_pretrained(\"bert-base-multilingual-uncased\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a408a4c5-cc5e-4454-8913-c0b781aeee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b697ba1b-8a27-4e11-970c-c92e0b29c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Outputs:\n",
    "  - last_hidden_state: hidden states of the last encoder block\n",
    "    - shape = (batch_size, max_length, 768)\n",
    "  - pooler_output: fc(last_hidden_state)\n",
    "    - shape = (batch_size, 768)\n",
    "  - hidden_states: hidden states of all encoder blocks\n",
    "    - length = 13 (note: bert uses 13 encoder blocks)\n",
    "    - shape of each hidden states = (batch_size, max_length, 768)\n",
    "\"\"\"\n",
    "\n",
    "X_features = {}\n",
    "for data_type, inputs in X_tokenized.items():\n",
    "    num_samples = inputs[\"input_ids\"].shape[0]\n",
    "    num_iter = num_samples // batch_size + 1\n",
    "    outputs = {\"last_hidden_state\": None, \"pooler_output\": None}\n",
    "    for i in range(num_iter):\n",
    "        s, e = i*batch_size, (i+1)*batch_size\n",
    "        cur_inputs = {k: v[s:e] for k, v in inputs.items()}\n",
    "\n",
    "        # batch inference\n",
    "        cur_outputs = model(cur_inputs)\n",
    "\n",
    "        # concat outputs\n",
    "        for k, v in outputs.items():\n",
    "            if v is None:                \n",
    "                outputs[k] = cur_outputs[k]\n",
    "            else:\n",
    "                outputs[k] = tf.concat([v, cur_outputs[k]], axis=0)\n",
    "    \n",
    "    X_features[data_type] = outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f00831-49af-489b-9734-5a19ea42c3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa724032-9112-45ce-aa56-794e67ec77a8",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e788f3c9-be27-4d4d-987d-3a5123d56a81",
   "metadata": {},
   "source": [
    "### Use Last Hidden States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a7c3cc1-7d92-48c0-a74c-c2f5c00dabdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (6400, 768)\n",
      "y_train: (6400,)\n",
      "X_test: (2000, 768)\n",
      "y_test: (2000,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE:\n",
    "  - last_hidden_state.shape = (batch_size, tokens, 768)\n",
    "  - first token is [CLS], which contains overall information about the input sentence\n",
    "  - therefore, we only use the first token (=[CLS]) as the input features\n",
    "\"\"\"\n",
    "\n",
    "X_train = X_features[\"X_train\"][\"last_hidden_state\"][:, 0, :]\n",
    "y_train = y_train_raw\n",
    "\n",
    "X_test = X_features[\"X_test\"][\"last_hidden_state\"][:, 0, :]\n",
    "y_test = y_test_raw\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7745e11-b665-4715-acf4-58d85f3477d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10000, solver='saga')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=10000, solver=\"saga\")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7310fcd-9f0e-4438-addb-63c34ab46f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = clf.predict(X_train)\n",
    "pred_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "463f3d47-9c18-4591-9220-f5606a0fd496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.71      0.71      3051\n",
      "           1       0.74      0.75      0.74      3349\n",
      "\n",
      "    accuracy                           0.73      6400\n",
      "   macro avg       0.73      0.73      0.73      6400\n",
      "weighted avg       0.73      0.73      0.73      6400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "841525d5-278b-4add-93fb-8a45cf68f84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.68      0.68       935\n",
      "           1       0.72      0.71      0.71      1065\n",
      "\n",
      "    accuracy                           0.70      2000\n",
      "   macro avg       0.70      0.70      0.70      2000\n",
      "weighted avg       0.70      0.70      0.70      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f468e9b5-5ef6-44fa-bc26-25098c030d61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f8d2875-c9aa-40ab-8667-c4bacb6c66d6",
   "metadata": {},
   "source": [
    "### Use Pooler Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce9e88f0-9719-4308-8464-15687a9d841d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (6400, 768)\n",
      "y_train: (6400,)\n",
      "X_test: (2000, 768)\n",
      "y_test: (2000,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE:\n",
    "  - last_hidden_state.shape = (batch_size, tokens, 768)\n",
    "  - first token is [CLS], which contains overall information about the input sentence\n",
    "  - therefore, we only use the first token (=[CLS]) as the input features\n",
    "\"\"\"\n",
    "\n",
    "X_train = X_features[\"X_train\"][\"pooler_output\"]\n",
    "y_train = y_train_raw\n",
    "\n",
    "X_test = X_features[\"X_test\"][\"pooler_output\"]\n",
    "y_test = y_test_raw\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "\n",
    "print(f\"X_test: {X_test.shape}\")\n",
    "print(f\"y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a753e62b-3b8b-43c3-abd2-9d85411163fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=10000, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=10000, solver='saga')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(max_iter=10000, solver=\"saga\")\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3cf0ba3-8737-4462-a4f7-c1c675501b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = clf.predict(X_train)\n",
    "pred_test = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "815f17bd-2a3d-47e0-ac69-793458c8691a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.65      0.66      3051\n",
      "           1       0.69      0.70      0.70      3349\n",
      "\n",
      "    accuracy                           0.68      6400\n",
      "   macro avg       0.68      0.68      0.68      6400\n",
      "weighted avg       0.68      0.68      0.68      6400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_train, pred_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12a07ed5-d4f8-4c29-a279-625234d8a995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.62      0.63       935\n",
      "           1       0.67      0.68      0.68      1065\n",
      "\n",
      "    accuracy                           0.65      2000\n",
      "   macro avg       0.65      0.65      0.65      2000\n",
      "weighted avg       0.65      0.65      0.65      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951aeb22-f69d-4456-a706-9b1ab818a06a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c400daf-5f32-4b70-bc17-c2b8125abf90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9103ae63-0c17-46a7-ae1b-7fe638a8f5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4de0f5-4c89-4460-9eed-6018041a5416",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
