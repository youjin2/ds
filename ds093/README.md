# DS093
## Introduction
A collection of examples in Data Science Series 093. (Python Text Mining Bible)


## Papers
- [Efficient Estimation of Word Representations in Vector Space] (Word2Vec)
- [Distributed Representations of Words and Phrases and their Compositionality] (Word2Vec + Negative Sampling)
- [Enriching Word Vectors with Subword Information] (FastText)
- [Distributed Representations of Sentences and Documents] (Doc2Vec)
- [Neural Machine Translation by Jointly Learning to Align and Translate] (Attention)
- [Attention Is All You Need] (Transformer)
- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding] (BERT)
- [ALBERT: A Lite BERT for Self-supervised Learning of Language Representations] (ALBERT)
- [SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing] (SentencePiece)
- [RoBERTa: A Robustly Optimized BERT Pretraining Approach] (RoBERTa)
- [ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators] (ELECTRA)
- [Distilling the Knowledge in a Neural Network] (Knowledge Distillation)
- [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter] (DistilBERT)
- [TinyBERT: Distilling BERT for Natural Language Understanding] (TinyBERT)
- [Improving Language Understanding by Generative Pre-Training] (GPT-1)
- [Language Models are Unsupervised Multitask Learners] (GPT-2)
- [Language Models are Few-Shot Learners] (GPT-3)
- [Training language models to follow instructions with human feedback] (InstructGPT)




## References
- [Text Mining Bible]
- [Tensorflow NLP Tutorial]


[Text Mining Bible]: https://github.com/wikibook/text-mining-bible
[Tensorflow NLP Tutorial]: https://wikidocs.net/book/2155

[Efficient Estimation of Word Representations in Vector Space]:https://arxiv.org/abs/1301.3781
[Distributed Representations of Words and Phrases and their Compositionality]: https://arxiv.org/abs/1310.4546
[Enriching Word Vectors with Subword Information]: https://arxiv.org/abs/1607.04606
[Distributed Representations of Sentences and Documents]: https://arxiv.org/abs/1405.4053
[Attention Is All You Need]: https://arxiv.org/abs/1706.03762
[Neural Machine Translation by Jointly Learning to Align and Translate]: https://arxiv.org/abs/1409.0473
[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding]: https://arxiv.org/abs/1810.04805
[ALBERT: A Lite BERT for Self-supervised Learning of Language Representations]: https://arxiv.org/abs/1909.11942
[SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing]: https://arxiv.org/abs/1808.06226
[RoBERTa: A Robustly Optimized BERT Pretraining Approach]: https://arxiv.org/abs/1907.11692
[ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators]: https://arxiv.org/abs/2003.10555
[Distilling the Knowledge in a Neural Network]: https://arxiv.org/abs/1503.02531
[DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter]: https://arxiv.org/abs/1910.01108
[TinyBERT: Distilling BERT for Natural Language Understanding]: https://arxiv.org/abs/1909.10351
[Improving Language Understanding by Generative Pre-Training]: https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
[Language Models are Unsupervised Multitask Learners]: https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
[Language Models are Few-Shot Learners]: https://arxiv.org/abs/2005.14165
[Training language models to follow instructions with human feedback]: https://arxiv.org/abs/2203.02155
